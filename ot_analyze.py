import json
import os
import shutil
import subprocess
import tempfile
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from enum import Enum, auto
from pathlib import Path
from typing import List

from write_failed_analysis import write_failed_analysis


ZIP_FILE_BASENAME = "protocols_and_analyses"


class OutputType(Enum):
    NONE = auto()
    ZIP = auto()
    MARKDOWN = auto()


def generate_analysis_path(protocol_file: Path) -> Path:
    """
    Takes a Path to a protocol file and returns a Path to the analysis file that
    should be generated by the analyze command.

    :param protocol_file: A Path to a protocol file.
    :return: A Path to the analysis file that should be generated by the analyze command.
    """
    return Path(protocol_file.parent, f"{protocol_file.stem}_analysis.json")


def analyze(protocol_file: Path):
    start_time = time.time()  # Start timing
    analysis_file = generate_analysis_path(protocol_file)
    custom_labware_directory = Path(protocol_file.parent, "custom_labware")

    custom_labware = []
    # PD protocols contain their own custom labware
    if custom_labware_directory.is_dir() and protocol_file.suffix == ".py":
        custom_labware = [
            os.path.join(custom_labware_directory, file) for file in os.listdir(custom_labware_directory) if file.endswith(".json")
        ]

    command = [
        "python",
        "-I",
        "-m",
        "opentrons.cli",
        "analyze",
        "--json-output",
        analysis_file,
        protocol_file,
    ] + custom_labware
    try:
        subprocess.run(command, capture_output=True, text=True, check=True)
    except Exception as e:
        print(f"Error in analysis of {protocol_file}")
        write_failed_analysis(analysis_file, e)
        end_time = time.time()
        return end_time - start_time
    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f"Successful analysis of {protocol_file} completed in {elapsed_time:.2f} seconds")
    return elapsed_time


def run_analyze_in_parallel(protocol_files: List[Path]):
    start_time = time.time()
    with ThreadPoolExecutor() as executor:
        futures = [executor.submit(analyze, file) for file in protocol_files]
        accumulated_time = 0
        for future in as_completed(futures):
            try:
                accumulated_time += future.result()  # This blocks until the future is done
            except Exception as e:
                print(f"An error occurred: {e}")
        end_time = time.time()
        clock_time = end_time - start_time
        print(
            f"""{len(protocol_files)} protocols with total analysis time of {accumulated_time:.2f}seconds.
Analyzed in {clock_time:2f} seconds thanks to parallelization.
"""
        )


def find_python_protocols(directory: Path) -> List[Path]:
    # Check if the provided path is a valid directory

    if not directory.is_dir():
        raise NotADirectoryError(f"The path {directory} is not a valid directory.")

    # Recursively find all .py files
    python_files = list(directory.rglob("*.py"))
    # TODO: shallow test that they are valid protocol files
    return python_files


def has_designer_application(json_file_path):
    try:
        with open(json_file_path, "r", encoding="utf-8") as file:
            data = json.load(file)
            return "designerApplication" in data
    except json.JSONDecodeError:
        # Handle the exception if the file is not a valid JSON
        print(f"Invalid JSON file: {json_file_path}")
        return False


def find_pd_protocols(directory: Path) -> List[Path]:
    # Check if the provided path is a valid directory
    if not directory.is_dir():
        raise NotADirectoryError(f"The path {directory} is not a valid directory.")

    # Recursively find all .json files
    json_files = list(directory.rglob("*.json"))
    filtered_json_files = [file for file in json_files if has_designer_application(file)]
    return filtered_json_files

def get_output_type() -> OutputType:
    """Get the output type from the environment variable OUTPUT_TYPE"""
    if os.getenv("OUTPUT_TYPE") == "markdown":
        output_type = OutputType.MARKDOWN
    elif os.getenv("OUTPUT_TYPE") == "zip":
        output_type = OutputType.ZIP
    elif os.getenv("OUTPUT_TYPE") == "none":
        output_type = OutputType.NONE
    else:
        print(f'Invalid OUTPUT_TYPE: {os.getenv("OUTPUT_TYPE")}. Defaulting to "none"')
        output_type = OutputType.NONE
    return output_type



def create_zip(directory_path: Path):
    absolute_directory_path = directory_path.absolute()
    try:
        archive_name = shutil.make_archive(ZIP_FILE_BASENAME, 'zip', absolute_directory_path, absolute_directory_path)
        print(f"Zipfile created and saved to: {absolute_directory_path / archive_name}")

    except Exception as e:
        print(f"Error: {e}")


def main():
    repo_relative_path = Path(os.getenv("GITHUB_WORKSPACE"), os.getenv("INPUT_BASE_DIRECTORY"))
    output_type = get_output_type()
    print(f"Using output type: {output_type.name.lower()}")
    print(f"Analyzing all protocol files in {repo_relative_path}")
    python_files = find_python_protocols(repo_relative_path)
    pd_files = find_pd_protocols(repo_relative_path)
    all_protocol_files = python_files + pd_files
    run_analyze_in_parallel(all_protocol_files)

    if output_type == OutputType.ZIP:
        create_zip(repo_relative_path)



if __name__ == "__main__":
    main()
